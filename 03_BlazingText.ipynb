{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BlazingText\n",
    "\n",
    "In this example we take the labelled set of news articles and use Amazon Blazing Text to build a multi-class classifier. We will evaluate it on an independent holdout set and look at accuracy includeing the top predicted topic, and the top two predicted topics.\n",
    "\n",
    "This Notebook was run in Sagemaker Studio with The **Python 3 (Data Science)** Kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::320389841409:role/service-role/AmazonSageMaker-ExecutionRole-20201022T141998\n",
      "sagemaker-ap-southeast-2-320389841409\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"funnybones\"\n",
    "prefix = \"rural/topics/blazingtext\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import shuffle\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import csv\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab the training and testing datasets\n",
    "\n",
    "These were prepped in the EDA Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"data/test.csv\"\n",
    "test_df = pd.read_csv(test_file, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"data/training.csv\"\n",
    "train_df = pd.read_csv(train_file, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_instance(row):\n",
    "    cur_row = []\n",
    "    # Prefix the index-ed label with __label__\n",
    "    label = \"__label__\" + row[0]\n",
    "    cur_row.append(label)\n",
    "    cur_row.extend(nltk.word_tokenize(row[1].lower()))\n",
    "    return cur_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_file, output_file, keep=1):\n",
    "    all_rows = []\n",
    "    with open(input_file, \"r\") as csvinfile:\n",
    "        csv_reader = csv.reader(csvinfile, delimiter=\",\")\n",
    "        for row in csv_reader:\n",
    "            all_rows.append(row)\n",
    "    shuffle(all_rows)\n",
    "    all_rows = all_rows[: int(keep * len(all_rows))]\n",
    "    pool = Pool(processes=multiprocessing.cpu_count())\n",
    "    transformed_rows = pool.map(transform_instance, all_rows)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    with open(output_file, \"w\") as csvoutfile:\n",
    "        csv_writer = csv.writer(csvoutfile, delimiter=\" \", lineterminator=\"\\n\")\n",
    "        csv_writer.writerows(transformed_rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 202 ms, sys: 77.5 ms, total: 280 ms\n",
      "Wall time: 4.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Preparing the training dataset\n",
    "\n",
    "# Since preprocessing the whole dataset might take a couple of mintutes,\n",
    "# we keep 20% of the training dataset for this demo.\n",
    "# Set keep to 1 if you want to use the complete dataset\n",
    "preprocess(train_file, \"blazingtext.train\", keep=1)\n",
    "\n",
    "# Preparing the validation dataset\n",
    "preprocess(test_file, \"blazingtext.test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 60 blazingtext.train > blazingtext.validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -n +61 blazingtext.train > blazingtext.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    887  584716 3154512 blazingtext.training\n"
     ]
    }
   ],
   "source": [
    "!wc blazingtext.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.6 ms, sys: 12.2 ms, total: 69.7 ms\n",
      "Wall time: 384 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_channel = prefix + \"/train\"\n",
    "validation_channel = prefix + \"/validation\"\n",
    "\n",
    "sess.upload_data(path=\"blazingtext.training\", bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path=\"blazingtext.validation\", bucket=bucket, key_prefix=validation_channel)\n",
    "\n",
    "s3_train_data = \"s3://{}/{}\".format(bucket, train_channel)\n",
    "s3_validation_data = \"s3://{}/{}\".format(bucket, validation_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://funnybones/rural/topics/blazingtext/train'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = \"s3://{}/{}/output\".format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: latest.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker BlazingText container: 544295431143.dkr.ecr.ap-southeast-2.amazonaws.com/blazingtext:1 (ap-southeast-2)\n"
     ]
    }
   ],
   "source": [
    "container = sagemaker.image_uris.retrieve( \"blazingtext\", region_name, \"latest\")\n",
    "print(\"Using SageMaker BlazingText container: {} ({})\".format(container, region_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c4.4xlarge\",\n",
    "    volume_size=30,\n",
    "    max_run=432000,\n",
    "    input_mode=\"File\",\n",
    "    output_path=s3_output_location,\n",
    "    hyperparameters={\n",
    "        \"mode\": \"supervised\",\n",
    "        \"epochs\": 5000,\n",
    "        \"min_count\": 2,\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"vector_dim\": 50,\n",
    "        \"early_stopping\": True,\n",
    "        \"patience\": 14,\n",
    "        \"min_epochs\": 2000,\n",
    "        \"word_ngrams\": 2,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.inputs.TrainingInput(\n",
    "    s3_train_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/plain\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "validation_data = sagemaker.inputs.TrainingInput(\n",
    "    s3_validation_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/plain\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "data_channels = {\"train\": train_data, \"validation\": validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-06 06:20:18 Starting - Starting the training job...\n",
      "2021-07-06 06:20:20 Starting - Launching requested ML instancesProfilerReport-1625552418: InProgress\n",
      "...\n",
      "2021-07-06 06:21:11 Starting - Preparing the instances for training.........\n",
      "2021-07-06 06:22:47 Downloading - Downloading input data\n",
      "2021-07-06 06:22:47 Training - Training image download completed. Training in progress...\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[07/06/2021 06:22:48 WARNING 139773318444416] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[07/06/2021 06:22:48 WARNING 139773318444416] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[07/06/2021 06:22:48 INFO 139773318444416] nvidia-smi took: 0.025355100631713867 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[07/06/2021 06:22:48 INFO 139773318444416] Running single machine CPU BlazingText training using supervised mode.\u001b[0m\n",
      "\u001b[34mNumber of CPU sockets found in instance is  1\u001b[0m\n",
      "\u001b[34m[07/06/2021 06:22:48 INFO 139773318444416] Processing /opt/ml/input/data/train/blazingtext.training . File size: 3.0083770751953125 MB\u001b[0m\n",
      "\u001b[34m[07/06/2021 06:22:48 INFO 139773318444416] Processing /opt/ml/input/data/validation/blazingtext.validation . File size: 0.20688343048095703 MB\u001b[0m\n",
      "\u001b[34mRead 0M words\u001b[0m\n",
      "\u001b[34mNumber of words:  16242\u001b[0m\n",
      "\u001b[34mLoading validation data from /opt/ml/input/data/validation/blazingtext.validation\u001b[0m\n",
      "\u001b[34mLoaded validation data.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 6\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 12\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 18\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 24\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 30\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 35\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 41\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 47\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 53\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 59\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 65\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 71\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 77\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 83\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 89\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 94\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0196  Progress: 2.01%  Million Words/sec: 34.33 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 100\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 106\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 111\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 117\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 123\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 128\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 134\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 140\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 146\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 152\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 158\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 164\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 170\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 176\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 181\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 187\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 193\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 199\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 204\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 210\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 216\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 222\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 228\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 234\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 240\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 246\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 252\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 258\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 264\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 270\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 276\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 282\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 288\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 293\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 299\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 305\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 311\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 317\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 323\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 329\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 335\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 341\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 347\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0186  Progress: 7.06%  Million Words/sec: 34.26 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 352\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 358\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 364\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 370\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 376\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 382\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 388\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 394\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 400\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 406\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 412\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 417\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 423\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 429\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 435\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 441\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 447\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 453\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 459\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 465\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 471\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 477\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 482\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 488\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 494\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 500\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 506\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 512\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 518\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 524\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 530\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 536\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 542\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 548\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 554\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 560\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 566\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 572\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 577\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 583\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 589\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 595\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 601\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0176  Progress: 12.15%  Million Words/sec: 34.38 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 607\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 613\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 619\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 625\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 631\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 637\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 643\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 648\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 654\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 660\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 666\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 672\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 677\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 683\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 689\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 694\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 699\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 705\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 710\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 716\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 722\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 728\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 734\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 740\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 746\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 752\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 758\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 764\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 770\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 776\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 781\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 787\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 793\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 799\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 805\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 811\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 817\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 823\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 829\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 834\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 840\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 846\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 852\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0166  Progress: 17.16%  Million Words/sec: 34.27 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 858\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 863\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 869\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 875\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 881\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 887\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 893\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 898\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 904\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 910\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 916\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 921\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 927\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 933\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 939\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 944\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 950\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 956\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 962\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 967\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 973\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 979\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 985\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 991\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 997\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1003\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1008\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1014\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1020\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1025\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1031\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1036\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1042\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1048\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1053\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1058\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1064\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1070\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1075\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1080\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1086\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1092\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1097\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1103\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0156  Progress: 22.17%  Million Words/sec: 34.00 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1108\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1114\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1120\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1125\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1131\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1136\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1142\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1148\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1154\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1159\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1165\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1171\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1176\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1182\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1187\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1193\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1199\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1205\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1210\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1216\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1222\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1228\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1234\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1240\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1246\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1252\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1258\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1264\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1270\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1276\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1282\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1288\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1294\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1300\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1306\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1312\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1318\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1324\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1330\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1336\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1342\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1348\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1354\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0146  Progress: 27.20%  Million Words/sec: 34.02 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1359\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1365\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1371\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1377\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1383\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1389\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1395\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1401\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1407\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1413\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1419\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1425\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1431\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1437\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1443\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1448\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1454\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1460\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1466\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1472\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1478\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1484\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1490\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1496\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1502\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1508\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1514\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1520\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1526\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1532\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1538\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1544\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1550\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1556\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1562\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1568\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1574\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1580\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1586\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1592\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1598\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1604\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0136  Progress: 32.21%  Million Words/sec: 34.14 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1610\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1616\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1622\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1628\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1634\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1640\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1646\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1652\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1658\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1664\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1670\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1676\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1682\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1688\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1694\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1700\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1706\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1712\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1718\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1724\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1730\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1736\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1742\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1748\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1754\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1760\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1766\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1772\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1778\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1784\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1790\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1796\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1802\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1808\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1813\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1819\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1825\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1831\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1837\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1843\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1849\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1855\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0126  Progress: 37.23%  Million Words/sec: 34.23 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1861\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1867\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1873\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1879\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1885\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1891\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1897\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1903\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1908\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1914\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1920\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1925\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1931\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1937\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1942\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1948\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1954\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1960\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1966\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1972\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1977\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1983\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1989\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 1995\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 2001\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.65\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 2002\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.65\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 2006\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.666667\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 2011\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.666667\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 2014\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.683333\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 2019\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.65\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 2021\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.65\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 2 epochs.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 2022\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.65\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 3 epochs.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 2026\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.666667\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 4 epochs.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 2028\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.666667\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 5 epochs.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 2029\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.666667\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 6 epochs.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 2033\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.666667\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 7 epochs.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 2035\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.65\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 8 epochs.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 2038\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.65\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 9 epochs.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 2041\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.65\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 10 epochs.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 2043\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.666667\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 11 epochs.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 2046\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.666667\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 12 epochs.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 2048\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.666667\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 13 epochs.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 2052\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.666667\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 14 epochs.\u001b[0m\n",
      "\u001b[34mReached patience. Terminating training.\u001b[0m\n",
      "\u001b[34mBest epoch: 2014\u001b[0m\n",
      "\u001b[34mBest validation accuracy: 0.683333\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0000  Progress: 100.00%  Million Words/sec: 77.73 #####\n",
      "\u001b[0m\n",
      "\u001b[34mTraining finished.\u001b[0m\n",
      "\u001b[34mAverage throughput in Million words/sec: 77.73\u001b[0m\n",
      "\u001b[34mTotal training time in seconds: 37.67\n",
      "\u001b[0m\n",
      "\u001b[34m#train_accuracy: 1\u001b[0m\n",
      "\u001b[34mNumber of train examples: 887\n",
      "\u001b[0m\n",
      "\u001b[34m#validation_accuracy: 0.6833\u001b[0m\n",
      "\u001b[34mNumber of validation examples: 60\u001b[0m\n",
      "\n",
      "2021-07-06 06:23:44 Uploading - Uploading generated training model\n",
      "2021-07-06 06:24:44 Completed - Training job completed\n",
      "Training seconds: 120\n",
      "Billable seconds: 120\n",
      "CPU times: user 775 ms, sys: 108 ms, total: 883 ms\n",
      "Wall time: 4min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "bt_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment and scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "text_classifier = bt_model.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.m4.xlarge\", serializer=JSONSerializer()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model on the independent set\n",
    "\n",
    "This is data from a different newspaper, but categorised in the same way in the same human labelling process. This gives us a stronger sense of how the model will perform on **new** data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(test_df[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = list(test_df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the same nltk tokenizer that we used during data preparation for training\n",
    "tokenized_sentences = [\" \".join(nltk.word_tokenize(sent)) for sent in sentences]\n",
    "\n",
    "payload = {\"instances\": tokenized_sentences, \"configuration\": {\"k\": 2}}\n",
    "\n",
    "response = text_classifier.predict(payload)\n",
    "predictions = json.loads(response)\n",
    "#print(json.dumps(predictions, indent=2))\n",
    "\n",
    "preds = []\n",
    "seconds = []\n",
    "for elem in predictions:\n",
    "    preds.append(elem[\"label\"][0][9:])\n",
    "    seconds.append(elem[\"label\"][1][9:])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\"target\":categories, \"pred\":preds, \"second\":seconds})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"correct\"] = np.where(results[\"target\"]==results[\"pred\"],1,0)\n",
    "results[\"correct_in2\"] = np.where( (results[\"correct\"]==1) | (results[\"target\"]==results[\"second\"]),1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "      <th>second</th>\n",
       "      <th>correct</th>\n",
       "      <th>correct_in2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>business</td>\n",
       "      <td>realestate</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>politics</td>\n",
       "      <td>sport</td>\n",
       "      <td>realestate</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>health</td>\n",
       "      <td>health</td>\n",
       "      <td>business</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lifestyle</td>\n",
       "      <td>business</td>\n",
       "      <td>realestate</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>environment</td>\n",
       "      <td>business</td>\n",
       "      <td>society</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>business</td>\n",
       "      <td>realestate</td>\n",
       "      <td>business</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>human</td>\n",
       "      <td>crime</td>\n",
       "      <td>accident</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>business</td>\n",
       "      <td>business</td>\n",
       "      <td>sport</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sport</td>\n",
       "      <td>arts</td>\n",
       "      <td>society</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sport</td>\n",
       "      <td>sport</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sport</td>\n",
       "      <td>sport</td>\n",
       "      <td>arts</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sport</td>\n",
       "      <td>arts</td>\n",
       "      <td>environment</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sport</td>\n",
       "      <td>sport</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sport</td>\n",
       "      <td>sport</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sport</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>sport</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sport</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>politics</td>\n",
       "      <td>politics</td>\n",
       "      <td>business</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>arts</td>\n",
       "      <td>arts</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>health</td>\n",
       "      <td>health</td>\n",
       "      <td>business</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sport</td>\n",
       "      <td>environment</td>\n",
       "      <td>arts</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         target         pred       second  correct  correct_in2\n",
       "0      business     business   realestate        1            1\n",
       "1      politics        sport   realestate        0            0\n",
       "2        health       health     business        1            1\n",
       "3     lifestyle     business   realestate        0            0\n",
       "4   environment     business      society        0            0\n",
       "5      business   realestate     business        0            1\n",
       "6         human        crime     accident        0            0\n",
       "7      business     business        sport        1            1\n",
       "8         sport         arts      society        0            0\n",
       "9         sport        sport    lifestyle        1            1\n",
       "10        sport        sport         arts        1            1\n",
       "11        sport         arts  environment        0            0\n",
       "12        sport        sport    lifestyle        1            1\n",
       "13        sport        sport    lifestyle        1            1\n",
       "14        sport    lifestyle        sport        0            1\n",
       "15        sport    lifestyle        human        0            0\n",
       "16     politics     politics     business        1            1\n",
       "17         arts         arts    lifestyle        1            1\n",
       "18       health       health     business        1            1\n",
       "19        sport  environment         arts        0            0"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first level accuracy\n",
      "0.37349397590361444\n",
      "second level accuracy\n",
      "0.5783132530120482\n"
     ]
    }
   ],
   "source": [
    "print(\"first level accuracy\")\n",
    "acc = sum(results[\"correct\"])/len(results)\n",
    "print(acc)\n",
    "\n",
    "print(\"second level accuracy\")\n",
    "acc = sum(results[\"correct_in2\"])/len(results)\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first level accuracy for sport\n",
      "0.40625\n",
      "second level accuracy for sport\n",
      "0.65625\n"
     ]
    }
   ],
   "source": [
    "# Just for sport\n",
    "\n",
    "sport_df = results[ results[\"target\"]==\"sport\" ]\n",
    "print(\"first level accuracy for sport\")\n",
    "acc = sum(sport_df[\"correct\"])/len(sport_df)\n",
    "print(acc)\n",
    "\n",
    "print(\"second level accuracy for sport\")\n",
    "acc = sum(sport_df[\"correct_in2\"])/len(sport_df)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results with params\n",
    "\n",
    "first level accuracy\n",
    "0.37349397590361444\n",
    "second level accuracy\n",
    "0.6024096385542169\n",
    "\n",
    "    hyperparameters={\n",
    "        \"mode\": \"supervised\",\n",
    "        \"epochs\": 2000,\n",
    "        \"min_count\": 2,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"vector_dim\": 30,\n",
    "        \"early_stopping\": True,\n",
    "        \"patience\": 10,\n",
    "        \"min_epochs\": 200,\n",
    "        \"word_ngrams\": 1,\n",
    "    },"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram 2 -- Lower learning\n",
    "\n",
    "    hyperparameters={\n",
    "        \"mode\": \"supervised\",\n",
    "        \"epochs\": 5000,\n",
    "        \"min_count\": 2,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"vector_dim\": 40,\n",
    "        \"early_stopping\": True,\n",
    "        \"patience\": 10,\n",
    "        \"min_epochs\": 1000,\n",
    "        \"word_ngrams\": 2,\n",
    "    },\n",
    "\n",
    "### Overall\n",
    "\n",
    "first level accuracy\n",
    "0.3855421686746988\n",
    "second level accuracy\n",
    "0.46987951807228917\n",
    "\n",
    "### Sport\n",
    "first level accuracy for sport\n",
    "0.53125\n",
    "second level accuracy for sport\n",
    "0.65625\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram 1 -- Longer training\n",
    "\n",
    "    hyperparameters={\n",
    "        \"mode\": \"supervised\",\n",
    "        \"epochs\": 5000,\n",
    "        \"min_count\": 2,\n",
    "        \"learning_rate\": 0.03,\n",
    "        \"vector_dim\": 50,\n",
    "        \"early_stopping\": True,\n",
    "        \"patience\": 12,\n",
    "        \"min_epochs\": 1000,\n",
    "        \"word_ngrams\": 1,\n",
    "    },\n",
    "\n",
    "### Overall\n",
    "first level accuracy\n",
    "0.37349397590361444\n",
    "second level accuracy\n",
    "0.5783132530120482\n",
    "\n",
    "### Sport\n",
    "first level accuracy for sport\n",
    "0.40625\n",
    "second level accuracy for sport\n",
    "0.65625\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-2:452832661640:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
